{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7115f08",
   "metadata": {},
   "source": [
    "# Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc45a1",
   "metadata": {},
   "source": [
    "#### import Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abf36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56a8a2",
   "metadata": {},
   "source": [
    "## CHANGE PATH FOR YOU MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5a6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define the paths to the files\n",
    "# gift_card_path = 'raw_data/amazon_reviews_us_Gift_Card_v1_00.tsv'\n",
    "# major_appliances_path = 'raw_data/amazon_reviews_us_Major_Appliances_v1_00.tsv'\n",
    "# shoes_path = 'raw_data/amazon_reviews_us_Shoes_v1_00.tsv'\n",
    "# electronics_path =  'raw_data/amazon_reviews_us_Electronics_v1_00.tsv'\n",
    "\n",
    "# # Read the files into dataframes\n",
    "# df_gift_card = pd.read_csv(gift_card_path, sep='\\t', error_bad_lines=False)\n",
    "# df_major_appliances = pd.read_csv(major_appliances_path, sep='\\t', error_bad_lines=False)\n",
    "# df_shoes = pd.read_csv(shoes_path, sep='\\t', error_bad_lines=False)\n",
    "# df_electronics = pd.read_csv(electronics_path, sep='\\t', error_bad_lines=False)\n",
    "\n",
    "\n",
    "# # Display the first few rows of each dataframe to verify\n",
    "# print(\"Gift Card Reviews DataFrame:\")\n",
    "# display(df_gift_card.head(1))\n",
    "# print(\"\\nMajor Appliances Reviews DataFrame:\")\n",
    "# display(df_major_appliances.head(1))\n",
    "# print(\"Shoes Reviews DataFrame:\")\n",
    "# display(df_shoes.head(1))\n",
    "# print(\"Electronics Reviews DataFrame:\")\n",
    "# display(df_electronics.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfc2ae",
   "metadata": {},
   "source": [
    "# Read Data & Filter Based On User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa813ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 19:28:49.832 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/Yoni/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-03-07 19:28:49.834 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2024-03-07 19:28:49.836 No runtime found, using MemoryCacheStorageManager\n",
      "2024-03-07 19:28:49.839 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2024-03-07 19:28:49.844 No runtime found, using MemoryCacheStorageManager\n",
      "2024-03-07 19:28:49.852 No runtime found, using MemoryCacheStorageManager\n",
      "2024-03-07 19:28:49.862 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Streamlit app for analyzing Amazon product reviews.\n",
    "\n",
    "How to use: Ensure the data files are in the same directory and pandas version 1.1.3 is installed.\n",
    "Open a terminal to this directory, and then run the command: streamlit run test_streamlit_v2_PEP8_kill_warning.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for the datasets\n",
    "DATASET_PATHS = {\n",
    "    'Gift Card': 'amazon_reviews_us_Gift_Card_v1_00.tsv',\n",
    "    'Major Appliances': 'amazon_reviews_us_Major_Appliances_v1_00.tsv',\n",
    "    'Shoes': 'amazon_reviews_us_Shoes_v1_00.tsv',\n",
    "    'Electronics': 'amazon_reviews_us_Electronics_v1_00.tsv'\n",
    "}\n",
    "\n",
    "@st.experimental_memo\n",
    "def read_dataset(dataset_path):\n",
    "    \"\"\"Reads a dataset from a given file path and returns a DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path, sep='\\t', on_bad_lines='skip')\n",
    "        return df, \"\"\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), str(e)\n",
    "\n",
    "@st.experimental_memo\n",
    "def merge_datasets(selected_category):\n",
    "    \"\"\"Merges datasets based on the selected category or all if 'all' is selected.\"\"\"\n",
    "    if selected_category in DATASET_PATHS:\n",
    "        df, error = read_dataset(DATASET_PATHS[selected_category])\n",
    "        return df, error\n",
    "    elif selected_category == 'all':\n",
    "        dfs = []\n",
    "        for path in DATASET_PATHS.values():\n",
    "            df, error = read_dataset(path)\n",
    "            if error:\n",
    "                return pd.DataFrame(), error\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs, ignore_index=True), \"\"\n",
    "\n",
    "def remove_specific_columns(df):\n",
    "    \"\"\"Removes specific columns from the DataFrame.\"\"\"\n",
    "    return df.drop(columns=[\"customer_id\", \"review_id\", \"product_id\"])\n",
    "\n",
    "def modify_review_date_to_year(df):\n",
    "    \"\"\"Converts the review_date in DataFrame to only include the year.\"\"\"\n",
    "    df['review_date'] = pd.to_datetime(df['review_date'], errors='coerce').dt.year\n",
    "    df.dropna(subset=['review_date'], inplace=True)\n",
    "    df['review_date'] = df['review_date'].astype(int)\n",
    "    return df\n",
    "\n",
    "def categorize_votes(df, column_names):\n",
    "    \"\"\"Categorizes votes into engagement levels.\"\"\"\n",
    "    for column in column_names:\n",
    "        category_col_name = f'{column}_category'\n",
    "        df[category_col_name] = 'No Votes'  # Default category for 0 votes\n",
    "        has_votes = df[column] > 0\n",
    "        votes_data = df.loc[has_votes, column]\n",
    "\n",
    "        try:\n",
    "            df.loc[has_votes, category_col_name] = pd.qcut(votes_data, q=4,\n",
    "                                                           labels=[\"Minimal Engagement\", \"Low Engagement\",\n",
    "                                                                   \"Moderate Engagement\", \"High Engagement\"],\n",
    "                                                           duplicates='drop')\n",
    "        except ValueError:\n",
    "            # Fallback strategy if qcut fails due to non-unique bin edges\n",
    "            bins = [votes_data.min(), votes_data.max()]\n",
    "            labels = [\"Engagement\"]\n",
    "            df.loc[has_votes, category_col_name] = pd.cut(votes_data, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Streamlit UI components\n",
    "def main():\n",
    "    \"\"\"Main function to render the Streamlit UI components.\"\"\"\n",
    "    st.title('Amazon Reviews Analysis')\n",
    "\n",
    "    selected_category = st.selectbox(\n",
    "        \"Please select a product category:\",\n",
    "        ('Gift Card', 'Major Appliances', 'Shoes', 'Electronics', 'all')\n",
    "    )\n",
    "\n",
    "    df, error = merge_datasets(selected_category)\n",
    "    if error:\n",
    "        st.error(f\"Failed to load dataset: {error}\")\n",
    "    else:\n",
    "        st.write(f\"Dataset for '{selected_category}' category loaded.\")\n",
    "        df = remove_specific_columns(df)\n",
    "        st.write(\"Removed specific columns: customer_id, review_id, product_id\")\n",
    "\n",
    "        df = modify_review_date_to_year(df)\n",
    "        st.write(\"Modified 'review_date' to retain the year only.\")\n",
    "\n",
    "        df = categorize_votes(df, ['helpful_votes', 'total_votes'])\n",
    "        st.write(\"Categorized 'helpful_votes' and 'total_votes' into engagement levels.\")\n",
    "\n",
    "        selected_variables = st.multiselect(\n",
    "            \"Select variables to filter by:\",\n",
    "            options=df.columns,\n",
    "            default=df.columns[0]\n",
    "        )\n",
    "\n",
    "        if selected_variables:\n",
    "            filters = {var: st.selectbox(f\"Values for {var}:\", ['All'] + list(df[var].dropna().unique()))\n",
    "                       for var in selected_variables}\n",
    "            for var, value in filters.items():\n",
    "                if value != 'All':\n",
    "                    df = df[df[var] == value]\n",
    "            st.write(\"Filtered dataset based on your selections:\")\n",
    "            st.dataframe(df.head())\n",
    "        else:\n",
    "            st.write(\"No variable selections made. Displaying first entries of the dataset.\")\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Streamlit app for analyzing Amazon product reviews.\n",
    "\n",
    "How to use: Ensure the data files are in the same directory and pandas version 1.1.3 is installed.\n",
    "Open a terminal to this directory, and then run the command: streamlit run test_streamlit_v2_PEP8_kill_warning.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for the datasets\n",
    "DATASET_PATHS = {\n",
    "    'Gift Card': 'amazon_reviews_us_Gift_Card_v1_00.tsv',\n",
    "    'Major Appliances': 'amazon_reviews_us_Major_Appliances_v1_00.tsv',\n",
    "    'Shoes': 'amazon_reviews_us_Shoes_v1_00.tsv',\n",
    "    'Electronics': 'amazon_reviews_us_Electronics_v1_00.tsv'\n",
    "}\n",
    "\n",
    "@st.experimental_memo\n",
    "def read_dataset(dataset_path):\n",
    "    \"\"\"Reads a dataset from a given file path and returns a DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path, sep='\\t', on_bad_lines='skip')\n",
    "        return df, \"\"\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame(), str(e)\n",
    "\n",
    "@st.experimental_memo\n",
    "def merge_datasets(selected_category):\n",
    "    \"\"\"Merges datasets based on the selected category or all if 'all' is selected.\"\"\"\n",
    "    if selected_category in DATASET_PATHS:\n",
    "        df, error = read_dataset(DATASET_PATHS[selected_category])\n",
    "        return df, error\n",
    "    elif selected_category == 'all':\n",
    "        dfs = []\n",
    "        for path in DATASET_PATHS.values():\n",
    "            df, error = read_dataset(path)\n",
    "            if error:\n",
    "                return pd.DataFrame(), error\n",
    "            dfs.append(df)\n",
    "        return pd.concat(dfs, ignore_index=True), \"\"\n",
    "\n",
    "def remove_specific_columns(df):\n",
    "    \"\"\"Removes specific columns from the DataFrame.\"\"\"\n",
    "    return df.drop(columns=[\"customer_id\", \"review_id\", \"product_id\"])\n",
    "\n",
    "def modify_review_date_to_year(df):\n",
    "    \"\"\"Converts the review_date in DataFrame to only include the year.\"\"\"\n",
    "    df['review_date'] = pd.to_datetime(df['review_date'], errors='coerce').dt.year\n",
    "    df.dropna(subset=['review_date'], inplace=True)\n",
    "    df['review_date'] = df['review_date'].astype(int)\n",
    "    return df\n",
    "\n",
    "def categorize_votes(df, column_names):\n",
    "    \"\"\"Categorizes votes into engagement levels.\"\"\"\n",
    "    for column in column_names:\n",
    "        category_col_name = f'{column}_category'\n",
    "        df[category_col_name] = 'No Votes'  # Default category for 0 votes\n",
    "        has_votes = df[column] > 0\n",
    "        votes_data = df.loc[has_votes, column]\n",
    "\n",
    "        try:\n",
    "            df.loc[has_votes, category_col_name] = pd.qcut(votes_data, q=4,\n",
    "                                                           labels=[\"Minimal Engagement\", \"Low Engagement\",\n",
    "                                                                   \"Moderate Engagement\", \"High Engagement\"],\n",
    "                                                           duplicates='drop')\n",
    "        except ValueError:\n",
    "            # Fallback strategy if qcut fails due to non-unique bin edges\n",
    "            bins = [votes_data.min(), votes_data.max()]\n",
    "            labels = [\"Engagement\"]\n",
    "            df.loc[has_votes, category_col_name] = pd.cut(votes_data, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Streamlit UI components\n",
    "def main():\n",
    "    \"\"\"Main function to render the Streamlit UI components.\"\"\"\n",
    "    st.title('Amazon Reviews Analysis')\n",
    "\n",
    "    selected_category = st.selectbox(\n",
    "        \"Please select a product category:\",\n",
    "        ('Gift Card', 'Major Appliances', 'Shoes', 'Electronics', 'all')\n",
    "    )\n",
    "\n",
    "    df, error = merge_datasets(selected_category)\n",
    "    if error:\n",
    "        st.error(f\"Failed to load dataset: {error}\")\n",
    "    else:\n",
    "        st.write(f\"Dataset for '{selected_category}' category loaded.\")\n",
    "        df = remove_specific_columns(df)\n",
    "        st.write(\"Removed specific columns: customer_id, review_id, product_id\")\n",
    "\n",
    "        df = modify_review_date_to_year(df)\n",
    "        st.write(\"Modified 'review_date' to retain the year only.\")\n",
    "\n",
    "        df = categorize_votes(df, ['helpful_votes', 'total_votes'])\n",
    "        st.write(\"Categorized 'helpful_votes' and 'total_votes' into engagement levels.\")\n",
    "\n",
    "        selected_variables = st.multiselect(\n",
    "            \"Select variables to filter by:\",\n",
    "            options=df.columns,\n",
    "            default=df.columns[0]\n",
    "        )\n",
    "\n",
    "        if selected_variables:\n",
    "            filters = {var: st.selectbox(f\"Values for {var}:\", ['All'] + list(df[var].dropna().unique()))\n",
    "                       for var in selected_variables}\n",
    "            for var, value in filters.items():\n",
    "                if value != 'All':\n",
    "                    df = df[df[var] == value]\n",
    "            st.write(\"Filtered dataset based on your selections:\")\n",
    "            st.dataframe(df.head())\n",
    "        else:\n",
    "            st.write(\"No variable selections made. Displaying first entries of the dataset.\")\n",
    "            st.dataframe(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee876131",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4543b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fb1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
